{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f425175a",
   "metadata": {},
   "source": [
    "## Introducing the royal family of vectors\n",
    "### A Basic Illustrative Example of Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b9b10",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "This notebook runs through a basic example of how to vectorize words from a pre-existing corpora to illustrate the vector algebra at the core of one possible utility of Word2Vec for social scientists. To do this, I rely on GenSim, an open source topic modelling library that can process raw and unstructured plain text files using unsupervised machine learning algorithms. Word2vec is one of the algorithms included in GenSim (others include Latent Semantic Indexing and Latent Dirichlet Allocation). The virtual environment to run this code is available on Github (gensimEnv.yml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55b5ee",
   "metadata": {},
   "source": [
    "#### Getting Started - From text to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dc40898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Word2Vec from GenSim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887f069",
   "metadata": {},
   "source": [
    "Once Word2Vec has been imported from Gensim, we can began to vectorize words. In this example, the corpora used to vectorize words is a pre-processed corpora on the Gensim database, called \"text8\". Text8 constitutes the first 100,000,000 bytes of plain text from Wikipedia, of which I only use a subset to speed up computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d12115b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading existing corpora from the Gensim database - Text from Wikipedia\n",
    "dataset = api.load(\"text8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fcfff1",
   "metadata": {},
   "source": [
    "The next step in the vectorization process is to create a list of words from the unstructured plain text file, a process known as tokenization. In reality, the tokenization produces a list of list, where each sentence is a list a words and each sentence is an element of the list that forms the original corpus. As previously mentioned, I subset the list to speed up computation in this example given the size of the corpora (\"text8\" is the first 100,000,000 bytes of plain text from Wikipedia). The unstructured plain text file has already been processed (the next demonstration runs through basic text processing, standard in natural language processing) and can simply be subsetted before being used to vectorize words with the Word2Vec algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4642710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing \n",
    "data = [d for d in dataset]\n",
    "data_part1 = data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2665e",
   "metadata": {},
   "source": [
    "#### Training the Word2Vec model- From tokens to vectors\n",
    "\n",
    "Now that we have \"tokenized\" our text, we are ready to vectorize words using the Word2Vec algorithm in GenSim. The default Word2Vec model is the Continuous Bag of Words (CBOW) model (see presentation for details), however it is trivial to change the model to Skip-gram (for instructions and additional details on the option ```sg``` [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)). Skip-gram has been argued to be much slower in training a Word2Vec model than CBOW, but has better training capibility using a smaller training corpus (see more details [here](https://arxiv.org/pdf/1301.3781.pdf)). In the code below, ```min_count``` tells the Word2Vec model to ignore the words that have a frequency less than the specified value (2, in this case), which should improve the model by removing rare words. The basic idea is that a vector representation of word that is rarely used in a training corpus will not have an accurate semantic representation. Further options can be added, such as specifying a desired vector dimension and specifying the way in which vectors are computed, but these are beyond the scope of this basic example (more details [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "298d374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Word2Vec model with text from Wikipedia (in the gensim database)\n",
    "word2vec = Word2Vec(data_part1, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646292a",
   "metadata": {},
   "source": [
    "#### Visualizing the Word2Vec model -  Words as Vectors\n",
    "We have now vectorized as set of words using their semantic context in a sample of Wikipedia articles. Obviously, these articles do not contain all of the words in the English language. In fact, we have \"only\" vectorized 100698 words and this constitutes the extent of our vocabulary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aecd3e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100698\n"
     ]
    }
   ],
   "source": [
    "vocab = len(word2vec.wv)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f3d23",
   "metadata": {},
   "source": [
    "As you can imagine, the larger your \"training\" corpus or corpora (equivalent to our \"text8\" in this example) the more words you can vectorize and generally the more accurate your model. More text means more semantic context through which the model can vectorize words and, therefore, the more accurate your vector represents the meaning of the word within your chosen text(s). Obviously, the choice of training corpus for the model should be dictated by the type of semantic analysis one wishes to embark upon, if you are using Word2Vec for semantic analysis (a more in-depth discussion [here](https://kbpedia.org/use-cases/document-specific-word2vec-training-corpuses/)). The vector for \"king\" can be displayed as follows and it is a 100-dimensional vector (vector dimensions can be changed in the options for the Word2Vec model, more details [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6116df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6247059 ,  2.734628  ,  0.07881403,  0.8415265 ,  1.0919182 ,\n",
       "       -1.1927114 , -0.15348631,  0.31121325,  1.2566534 , -0.67269063,\n",
       "        0.5604339 ,  2.6822858 ,  2.854661  ,  1.7861245 ,  3.8821514 ,\n",
       "       -1.1519864 ,  1.8963438 , -0.42539087,  2.2171638 , -0.6451796 ,\n",
       "        1.3156921 , -0.77337134,  2.1716301 ,  3.0620973 ,  2.2385871 ,\n",
       "        1.9413279 ,  0.76725775, -1.1715403 , -1.7636768 ,  0.4159524 ,\n",
       "       -0.37398598, -2.7588546 ,  0.58317375, -1.5069119 ,  1.7781116 ,\n",
       "        1.7118299 , -0.42223325, -1.4407419 ,  1.7727789 ,  1.5959058 ,\n",
       "       -0.94584805, -0.01704919, -0.263002  ,  2.6214325 ,  0.9325347 ,\n",
       "       -1.944263  , -3.248921  , -1.9091796 ,  1.0752041 ,  0.41822544,\n",
       "        3.2948973 ,  0.09252111, -3.8474638 , -2.503489  , -0.3280734 ,\n",
       "        0.8311117 ,  3.3224537 ,  0.52778506,  0.30486453,  1.3455604 ,\n",
       "        0.24585572,  0.9263525 ,  1.793085  , -1.0099862 ,  0.3908879 ,\n",
       "       -1.2896887 , -1.3128164 , -0.11316916, -0.41064888, -1.254687  ,\n",
       "        2.5540981 , -2.1212764 , -1.9593897 ,  1.3195864 ,  0.33235133,\n",
       "        1.4594887 ,  2.838726  , -0.72939336, -0.65724856,  0.7372532 ,\n",
       "        0.6839381 ,  1.0291715 ,  0.5574616 ,  1.6526141 ,  0.13806847,\n",
       "       -0.33238834, -0.01475193,  1.5813116 , -0.07260821,  1.6930803 ,\n",
       "        0.7732846 ,  0.8207004 , -2.3087122 , -1.3797399 , -1.9110872 ,\n",
       "        0.5951446 ,  1.8832892 ,  0.76675755, -2.1122017 , -2.0272765 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the vector for the word \"King\"\n",
    "word2vec.wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e6403",
   "metadata": {},
   "source": [
    "#### Semantic Similarity - From Vectors to Vector Algebra\n",
    "\n",
    "Given that we now have a set of words represented in vector format, we can assess the cosine similarity between these vectors as a way to assess semantic similarity. Following the premise in the presentation, the similarity between \"king\", \"queen\", \"male\" and \"female\" can be computed as follows. As expected, \"king\" and \"female\" are less semantically similar than \"king\" and \"male\".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9003f3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71888465"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarity between King and Queen\n",
    "word2vec.wv.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a5b3277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04135818"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarity between King and Female\n",
    "word2vec.wv.similarity('king', 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c4d7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.070132636"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarlity between King and Male \n",
    "word2vec.wv.similarity('king', 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f04fa94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92445123"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarity between Male and Female \n",
    "word2vec.wv.similarity('male', 'female')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb59ae",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "This notebook has covered the basics of Word2vec, showcasing how to vectorize words from a pre-processed sample of Wikipedia articles. This code was adapted from the following [tutorial](https://www.machinelearningplus.com/nlp/gensim-tutorial/#14howtotrainword2vecmodelusinggensim). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
